\section{Methodology}

\subsection{Theoretical framework}
%% Present the mathematical or theoretical foundation that underpins the new method. This might involve defining the equations or models you're improving.

Let's start with Schrödinger's time-independent equation:
%griffith - cytowanie, rozpoczęcie ze zwykłego równania Schrodingera, znalezienie hamiltonianiu
\begin{equation}
	\hat{H} \psi(x,y,z) = E \psi(x,y,z),
\end{equation}
where \(\hat{H}\) is a Hamiltonian operator, which in atomic units is defined as:
\begin{equation}
	\hat{H} = -\frac{1}{2} \Delta + \hat{V}(x,y,z),
\end{equation}
where the symbols have the meaning of:

\(\Delta\) : kinetic energy operator (or Laplace operator, or Laplacian)

\(\hat{V}(x,y,z)\) : potential energy operator

\(\psi(x,y,z)\) : spatial wavefunction, representing the quantum state

\(E\) : energy eigenvalue associated with the state \(\psi(x,y,z)\)

\noindent In the case of hydrogen atom, the Hamiltonian in atomic units simplifies to:
\begin{equation}
	\hat{H} = -\frac{1}{2}\Delta-\frac{1}{\sqrt{x^2+y^2+z^2}}
\end{equation}
where Laplacian in three dimensions is:

\begin{equation}
	\Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}
\end{equation}
The wavefunction is defined on the set of complex numbers, and because of Born's statistical interpretation, its integral spanning the real number continuum sums to 1:

\begin{equation}
	\boldsymbol{\psi}(x,y,z) \in \mathbb{C}
\end{equation}
\begin{equation}
	\int_{-\infty}^\infty\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\lvert \psi(x,y,z) \rvert^2 dx dy dz = 1
\end{equation}

\subsection{Sampling the wavefunction}

To begin with, let's assume this system can be scaled to large enough boundaries $<-A,-A>$, containing most of the wavefunction's probability density. The expected value of the Hamiltonian operator becomes:
\begin{equation}
	\int_{-A}^A\int_{-A}^{A}\int_{-A}^{A}\psi^{*}(x,y,z) H \psi(x,y,z)  dx dy dz = E
\end{equation}
The minimal energy (the energy of the ground state) of the hydrogen atom is equal to $-\frac{1}{2}$. The spectrum of the hydrogen atom
is degenerate, meaning that many different wavefunctions correspond to the
same energy level. As mentioned before, the general formula for the energy of the hydrogen atom
depending on the principal quantum number:
\begin{equation}
	E_n = -\frac{1}{2n^2}, \quad n \in \mathbb{N}
\end{equation}

%\subsection{Development of the new method}
%% Describe how you developed the new calculation method step by step. Include derivations, algorithms, or logic used.

\noindent Next, the wavefunction has to be sampled. Let us denote:
\begin{equation}
	x_i = -A + id
\end{equation}
\begin{equation}
	y_j = -A + jd
\end{equation}	
\begin{equation}
	z_k = -A + kd
\end{equation}	
where $d$ is the grid step. 
The wavefunction is defined only at the above discrete points. The sampled wavefunction can be written as:
\begin{equation}
	\psi_s(x,y,z) = \sum_{i=0}^{N-1}\sum_{j=0}^{N-1}\sum_{k=0}^{N-1}\psi(x_i,y_j,z_k)\delta(x-x_i)\delta(y-y_j)\delta(z-z_k),
\end{equation}
where $\delta$ is the Kronecker delta. % analiza sygnałów źródło
Denote:
\begin{equation}
	\phi(x_i,y_j,z_k) = -\frac{1}{2}\Delta\psi(x,y,z)\rvert_{x=x_i,y=y_j,z=z_k}-\frac{1}{\sqrt{x_i^2+y_j^2+z_k^2}}\psi(x_i,y_j,z_k).
\end{equation}
The discretized expected value of the Hamiltonian is equal:
\begin{equation}
	\sum_{i=0}^{N-1}\sum_{j=0}^{N-1}\sum_{k=0}^{N-1}\psi^{*}(x_i,y_j,z_k)\phi(x_i,y_j,z_k)d^3 = E.
\end{equation}

\subsection{Laplacian}
In order to discretize the Laplace operator, three separate methods were applied, one of which was a central difference scheme on a uniform structured mesh (CDS), as well as two high-order compact (HOC) finite difference schemes.\cite{spotz1996hoc}
Because of built-in CUDA features, such as fast reading of neighboring memory addresses
% < TODO cytowanie CUDA, akhtar>
they need to be as compact as possible. Because of the many points used, the accuracy of such stencils varies from $O(h^2)$ for the 7-point stencil to $O(h^6)$ for the 27-point stencil. Since the precision of this approximation of the Laplace operator is a function of the grid step, this provides further motivation to calculate with as small a grid step as possible, utilizing as much VRAM as possible.

For example, in the case of the 7 points stencil, the discrete Laplace operator acting on a wavefunction can be written as:

\begin{equation}
	\begin{aligned}
		\Delta \psi(x,y,z) \lvert_{x=x_i,y=y_j,z=z_k} =
		& \psi(x_{i+1},y_j,z_k) + \psi(x_{i-1},y_j,z_k) + \psi(x_i,y_{j+1},z_k) + \\
		& \psi(x_i,y_{j-1},z_k) + \psi(x_i,y_j,z_{k+1}) + \psi(x_i,y_j,z_{k-1}) \\
		& -6\psi(x_i,y_j,z_k)
	\end{aligned}
\end{equation}

This work utilized three stencils to calculate the Laplace operator: 7-point, 19-point, and 27-point. The weights of each stencil element are presented in Figure \ref{fig:stencils}.
The derivation of the formula for the 7-point stencil can be found in Appendix A.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{pictures/3/stencil7}
		\caption{7 points stencil}
		\label{fig:stencil7}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{pictures/3/stencil19}
		\caption{19 points stencil}
		\label{fig:stencil19}
	\end{subfigure}
	\hfill
	\bigskip
	
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{pictures/3/stencil27}
		\caption{27 points stencil}
		\label{fig:stencil27}
	\end{subfigure}
	\hfill
	
	\caption{CDS and HOC stencils described in William Spotz's dissertation. \cite{spotz1996hoc}}
	\label{fig:stencils}
\end{figure}


As Krotkiewski noted in his 2013 paper, those stencils are known as the general (non-separable) 3D convolution filters in computer graphics.\cite{krotkiewski2013efficient} 
% str. 5 rownanie 4




\subsection{Eigensolvers}
%TODO
Hereafter, and for the sake of simplicity, the sampled wavefunction flattened to a vector of N \times N \times N, \textbf{x} is defined to represent $\psi(x,y,z)$. What is left is calculating eigenvalues and eigenvectors using numerical methods. As stated before the time-independent Schr{\"o}dinger equation is:

\begin{equation}
	\hat{H} \psi(x,y,z) = E \psi(x,y,z)
\end{equation}

\noindent which is an eigenproblem,
%TODO
That is the transformation of wavefunction by Hamiltonian results in a scaled version. Because of discretization, the Hamiltonian can be treated as a matrix, and the wavefunction becomes a vector. To solve this for $E$, both sides of the equation are multiplied by $\psi^T(x,y,z).$

\begin{equation}
	\psi^T(x,y,z) \hat{H} \psi(x,y,z) = \psi^T(x,y,z) E \psi(x,y,z)
\end{equation}

\noindent note that $\psi^T(x,y,z)\psi(x,y,z)$ is a scalar, which means that E can be factored out on the right side

\begin{equation}
	\psi^T(x,y,z) \hat{H} \psi(x,y,z) = E(\psi^T(x,y,z) \psi(x,y,z))
\end{equation}

\noindent and both sides divided by also a scalar value of $\psi^T(x,y,z) \psi(x,y,z)$, which is an L2 norm of $\psi(x,y,z)$. After division, the equation becomes:

\begin{equation}
	E = \frac{\psi^T(x,y,z) \hat{H} \psi(x,y,z)}{\psi^T(x,y,z) \psi(x,y,z)}
\end{equation}

\noindent that is a Rayleigh's quotient, which is used in numerical calculations using computers since Vandergraft \cite{vandergraft1971} publication from 1971. Rayleigh's quotient can also be understood as a function that returns an eigenvalue when given a proper eigenvector.

\begin{equation}
	\lambda_i = R(\textbf{x}_i)
\end{equation}

\noindent

\subsubsection{Gradient descent}

Iterative methods were employed to solve the aforementioned quotient. The most basic algorithm, used for such tasks is gradient descent.\cite{gradient_descent} The algorithm idea is to have an initial guess of what the eigenvector might be. If no such information exists, an uniform random vector should be used. Then, the gradient of Rayleigh's quotient should be calculated. Since the gradient points in the direction where the function grows the fastest, and the algorithm tries to minimize the value, the gradient for the current $\textbf{x}$ iteration is calculated, negated, and multiplied by the learning rate. Thus, the next value of $\textbf{x}_{i+1}$ is:

\begin{equation}
	\textbf{x}_{i+1} = \textbf{x}_{i} - lr \cdot \nabla R(\textbf{x}_{i})
\end{equation}

\noindent, after which the check for convergence is made by checking if the L2 norm of the residue vector is greater than the assumed tolerance. Once the convergence criterion is satisfied or the maximum number of iterations is achieved, the procedure returns the eigenvalue and eigenvector. This algorithm enables the identification of the local minima of multivariate functions.

\subsubsection{Locally Optimal Block Preconditioned Conjugate Gradient}
%TODO
% Wikipedia, edit
% Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is a matrix-free method for finding the largest eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem. \cite{knyazew2001} Even though it is described as matrix-free, many implementations, such as one found in PyTorch, demand matrix operators.

Its implementations can be found in scientific modules such as SciPy \cite{scipy_lobpcg} and GPU-accelerated versions from PyTorch \cite{torch_lobpcg} and CuPyX \cite{cupy_lobpcg}.

\subsubsection{Steepest descent with optimal step}
%TODO
ŹRÓDŁO \cite{pgd}

The goal function is defined as
\begin{equation}
	\rho\left(\mathbf{x}, \bm{\lambda} \right) = \frac{\mathbf{x}^T\mathbf{A}\mathbf{x}}{\mathbf{x}^T\mathbf{x}} + \bm{\lambda}^T \mathbf{Y}^T \mathbf{x}, 
\end{equation}
where $\mathbf{Y}\in\mathbb{R}^{d \times k}$ contains orthogonal vectors defining the subspace orthogonal to the sought solution and $\bm{\lambda}\in\mathbb{R}^k$ is the vector of Lagrange multipliers.
Thus, the gradients read
\begin{equation}
	\nabla_{\mathbf{x}}\rho\left(\mathbf{x},\bm{\lambda}\right) = \frac{2}{\mathbf{x}^T\mathbf{x}}\left(\mathbf{A}-\frac{\mathbf{x}^T\mathbf{A}\mathbf{x}}{\mathbf{x}^T\mathbf{x}}\right)\mathbf{x} + \mathbf{Y}\bm{\lambda},
\end{equation}
\begin{equation}
	\nabla_{\bm{\lambda}}\rho\left(\mathbf{x},\bm{\lambda}\right) = \mathbf{Y}^T\mathbf{x}.
\end{equation}
The search direction for gradient descent is
\begin{equation}
	\mathbf{p}=-\left[\left(\nabla_{\mathbf{x}}\rho\left(\mathbf{x},\bm{\lambda}\right)\right)^T, \left(\nabla_{\bm{\lambda}}\rho\left(\mathbf{x},\bm{\lambda}\right)\right)^T\right]^T = \left[\mathbf{p}_{\mathbf{x}}^T,\mathbf{p}_{\bm{\lambda}}^T\right]^T.
\end{equation}

The goal as a function of the step size $\delta$ reads
\begin{equation}
	\rho\left(\mathbf{x}+\delta \mathbf{p}_{\mathbf{x}},\bm{\lambda}+\delta \mathbf{p}_{\bm{\lambda}}\right) = \frac{a_1+b_1\delta+c_1\delta^2}{a_2+b_2\delta+c_2\delta^2} + a_3+b_3\delta + c_3 \delta^2,
	\label{eq5}
\end{equation}
where
\begin{align*}
	a_1 &= \mathbf{x}^T\mathbf{A}\mathbf{x}, & \quad a_2 &= \mathbf{x}^T\mathbf{x}, & \quad a_3 &= \bm{\lambda}^T\mathbf{Y}^T\mathbf{x}, \\
	b_1 &= 2 \mathbf{p}_{\mathbf{x}}^T\mathbf{A}\mathbf{x}, & \quad b_2 &= 2\mathbf{p}_{\mathbf{x}}^T\mathbf{x}, & \quad b_3 &=  \mathbf{p}_{\bm{\lambda}} \mathbf{Y}^T \mathbf{x} + \bm{\lambda}\mathbf{Y}^T\mathbf{p}_{\mathbf{x}}, \\
	c_1 &= \mathbf{p}_{\mathbf{x}}^T \mathbf{A} \mathbf{p}_{\mathbf{x}}, & \quad c_2 &= \mathbf{p}_{\mathbf{x}}^T\mathbf{p}_{\mathbf{x}}, &  \quad c_3 &= \mathbf{p}_{\bm{\lambda}}^T\mathbf{Y}^T\mathbf{p}_{\mathbf{x}}.
\end{align*}
Minimization of \ref{eq5} w.r.t. $\delta$ leads to the $5$th order polynomial equation
\begin{equation}
	q_0 + q_1 \delta + q_2 \delta^2 + q_3 \delta^3 + q_4 \delta^4 +q_5 \delta^5 = 0,
\end{equation}
where the coefficients are:
\begin{align*}
	q_0 &= -a_1 b_2 + a_2 (b_1 + a_2 b_3),\\
	q_1 &= -2 a_1 c_2 + 2 a_2 (b_2 b_3 + c_1 + a_2 c_3),\\ 
	q_2 &= b_2^2 b_3 + b_2 c_1 - b_1 c_2 + 2 a_2 b_3 c_2 + 4 a_2 b_2 c_3, \\
	q_3 &= 2 (b_2 b_3 c_2 + b_2^2 c_3 + 2 a_2 c_2 c_3),\\
	q_4 &= c_2 (b_3 c_2 + 4 b_2 c_3),\\
	q_5 &= 2 c_2^2 c_3.
\end{align*}

which facilitates the calculation of the optimal step size.

\subsubsection{Adam}
%TODO

%\begin{aligned}
%	m_w^{(t+1)} = \beta_1 m_w^{(t)} + (1 - \beta_1) \nabla_w L^{(t)} \\
%	& v_w^{(t+1)} = \beta_2 v_w^{(t)} + (1 - \beta_2) (\nabla_w L^{(t)}) \\
%	& \hat{m_w} = frac{m_w^{(t+1)}}{1-\beta_1^t} \\
%	& \hat{v_w} = frac{v_w^{(t+1)}}{1-\beta_2^t} \\
%	& w^{(t+1)} = w^{(t)} - \eta frac{\hat{m_w}}{\sqrt{\hat{v_w}} + \epsilon}
%\end{aligne}
\cite{kingma_adam:_2017}

\subsection{Tools and techniques}

\subsubsection{Python}

\cite{mayavi}


\subsubsection{CuPy}

CuPy provides the functionality of NumPy and SciPy modules, offering GPU-accelerated computing with Python. It works on both Nvidia CUDA and AMD ROCm platforms. Its API is compatible with the aforementioned modules.\cite{cupy_overview}

CuPy also provides an easy way of implementing custom kernel functions. For example - taken from "Learn CUDA":\cite{learn_cuda}

\vspace{0.2cm}
\lstinputlisting[caption=Simple element-wise kernel, label=listing1, captionpos=b]{listings/3/listing1.py}
takes two arguments, x and y and the return value is stored in the variable z.

Also it is possible to use raw kernel functions. Raw kernels are used when one wants to define a custom kernel that executes CUDA source code. Using raw kernels allows for control of grid size, block size, shared memory size, and stream. An example of the raw kernel from CuPy docs\cite{cupy_raw_kernel}:

\vspace{0.2cm}
\lstinputlisting[caption=Example code of raw kernel that performs addition of two 2D arrays., label=listing2, captionpos=b]{listings/3/listing2.py}

Moreover, CuPy provided the TextureObject\cite{cupy_texture} and SurfaceObject\cite{cupy_surface}, allowing the CUDA textures and surfaces to be passed into the raw kernel. The relevance of this will be addressed in more depth in the next section. Also, the LOBPCG implementation from this module was used.\cite{cupy_lobpcg}


\subsubsection{CUDA}

In this work, we meet the case of data parallelism\cite{cheng2014professional}, which is a situation where calculation time might benefit when many data items are operated at the same time. The CUDA approach for such problems is to provide each thread with equal parts of data so that every thread can work on its part simultaneously.

An important issue with parallel computation is the 
Raw kernels were written using the technique described by Akhtar et al.\cite{akhtar2018efficient}. and Krotkiewski \cite{krotkiewski2013efficient}


% TODO opisać texturę i surface i guess
% TODO opisać benchmark obliczenia numerycznego

\cite{learn_cuda}
\cite{cheng2014professional}




